import streamlit as st
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer
from threading import Thread

# === 1. é¡µé¢é…ç½® ===
st.set_page_config(page_title="SuperChatbot Web", page_icon="ğŸ¤–", layout="wide")
st.title("ğŸ¤– SuperChatbot Pro (Webç‰ˆ)")

# === 2. åŠ è½½æ¨¡å‹ (ä½¿ç”¨ç¼“å­˜ï¼ŒåªåŠ è½½ä¸€æ¬¡) ===
@st.cache_resource
def load_model():
    model_name = "Qwen/Qwen2.5-0.5B-Instruct" # æ˜¾å­˜å¤Ÿå¯æ¢ 1.5B
    status_text = st.empty()
    status_text.info(f"æ­£åœ¨åŠ è½½æ¨¡å‹ {model_name}... è¯·ç¨å€™")
    
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype="auto",
        device_map="auto"
    )
    status_text.empty() # åŠ è½½å®Œæ¸…ç©ºæç¤º
    return tokenizer, model

try:
    tokenizer, model = load_model()
except Exception as e:
    st.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
    st.stop()

# === 3. ä¾§è¾¹æ ï¼šè®¾ç½®é¢æ¿ ===
with st.sidebar:
    st.header("âš™ï¸ å‚æ•°è®¾ç½®")
    
    # æ¨¡å¼é€‰æ‹©
    mode = st.radio("å¯¹è¯æ¨¡å¼", ["åŠ©æ‰‹æ¨¡å¼", "å°è¯´æ¨¡å¼"])
    
    # åŠ¨æ€å‚æ•°
    temperature = st.slider("æ¸©åº¦ (åˆ›é€ åŠ›)", 0.1, 1.5, 0.7 if mode == "åŠ©æ‰‹æ¨¡å¼" else 0.95)
    max_tokens = st.slider("æœ€å¤§å›å¤é•¿åº¦", 128, 2048, 1024)
    
    # æ¸…ç©ºæŒ‰é’®
    if st.button("ğŸ—‘ï¸ æ¸…ç©ºå¯¹è¯"):
        st.session_state.messages = []
        st.rerun()

# === 4. åˆå§‹åŒ–å¯¹è¯å†å² (Session State) ===
# Streamlit æ¯æ¬¡äº¤äº’éƒ½ä¼šé‡è·‘ä»£ç ï¼Œæ‰€ä»¥è¦å­˜åœ¨ session_state é‡Œ
if "messages" not in st.session_state:
    st.session_state.messages = []

# æ ¹æ®æ¨¡å¼è®¾ç½® System Prompt
if not st.session_state.messages:
    if mode == "å°è¯´æ¨¡å¼":
        sys_prompt = "ä½ æ˜¯ä¸€ä½è·å¾—è¯ºè´å°”æ–‡å­¦å¥–çš„å°è¯´å®¶ã€‚è¯·æ ¹æ®ç”¨æˆ·çš„è¦æ±‚åˆ›ä½œæƒ…èŠ‚è·Œå®•èµ·ä¼ã€æå†™ç»†è…»çš„å°è¯´ã€‚"
    else:
        sys_prompt = "ä½ æ˜¯ä¸€ä¸ªé€šæ™“ç™¾ç§‘ã€ä¹äºåŠ©äººçš„ä¸­æ–‡ AI åŠ©æ‰‹ã€‚"
    st.session_state.messages.append({"role": "system", "content": sys_prompt})

# === 5. æ¸²æŸ“å†å²å¯¹è¯ ===
for msg in st.session_state.messages:
    if msg["role"] == "user":
        st.chat_message("user").write(msg["content"])
    elif msg["role"] == "assistant":
        st.chat_message("assistant").write(msg["content"])

# === 6. å¤„ç†ç”¨æˆ·è¾“å…¥ ===
if user_input := st.chat_input("è¾“å…¥ä½ çš„é—®é¢˜..."):
    # æ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯
    st.chat_message("user").write(user_input)
    st.session_state.messages.append({"role": "user", "content": user_input})

    # === ç”Ÿæˆå›å¤ (æµå¼) ===
    with st.chat_message("assistant"):
        # æ„å»ºè¾“å…¥
        text = tokenizer.apply_chat_template(
            st.session_state.messages,
            tokenize=False,
            add_generation_prompt=True
        )
        model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

        # è®¾ç½®æµå¼è¾“å‡ºå™¨ (è¿™æ˜¯ Web ç‰ˆæµå¼çš„å…³é”®)
        streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)
        
        # å‚æ•°è®¾ç½®
        generation_kwargs = dict(
            model_inputs,
            streamer=streamer,
            max_new_tokens=max_tokens,
            do_sample=True,
            temperature=temperature,
            top_p=0.9,
            repetition_penalty=1.1
        )

        # åœ¨æ–°çº¿ç¨‹ä¸­è¿è¡Œç”Ÿæˆï¼Œé˜²æ­¢é˜»å¡ä¸»çº¿ç¨‹
        thread = Thread(target=model.generate, kwargs=generation_kwargs)
        thread.start()

        # st.write_stream ä¼šè‡ªåŠ¨ä» streamer è¯»å– tokens å¹¶æ‰“å­—æœºæ˜¾ç¤º
        response = st.write_stream(streamer)
        
        # å°†å®Œæ•´çš„å›å¤å­˜å…¥å†å²
        st.session_state.messages.append({"role": "assistant", "content": response})