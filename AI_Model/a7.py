import torch
import json
import os
import gc
from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer
from rich.console import Console
from rich.panel import Panel
from rich.markdown import Markdown
from rich.prompt import Prompt
from rich.text import Text

# åˆå§‹åŒ– Rich æ§åˆ¶å°
console = Console()

class SuperChatbot:
    def __init__(self, model_name="Qwen/Qwen2.5-1.5B-Instruct"):
        self.model_name = model_name
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.max_history_len = 10  # é™åˆ¶ä¿ç•™æœ€è¿‘çš„ N è½®å¯¹è¯ï¼Œé˜²æ­¢çˆ†æ˜¾å­˜
        
        console.print(f"[bold green]æ­£åœ¨åŠ è½½å¼•æ“: {self.model_name} (è®¾å¤‡: {self.device})...[/bold green]")
        
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                torch_dtype="auto",
                device_map="auto"
            )
        except Exception as e:
            console.print(f"[bold red]æ¨¡å‹åŠ è½½å¤±è´¥: {e}[/bold red]")
            console.print("è¯·æ£€æŸ¥ç½‘ç»œæˆ–æ˜¾å­˜ã€‚å»ºè®®å…ˆä½¿ç”¨è¾ƒå°çš„æ¨¡å‹å¦‚ 'Qwen/Qwen2.5-0.5B-Instruct'")
            exit()

        # é»˜è®¤ç”Ÿæˆå‚æ•°
        self.gen_kwargs = {
            "max_new_tokens": 1024,
            "do_sample": True,
            "temperature": 0.7,
            "top_p": 0.9,
            "repetition_penalty": 1.1
        }
        
        self.mode = "assistant"
        self.messages = []
        self.reset_history()

    def reset_history(self):
        """é‡ç½®å¯¹è¯"""
        if self.mode == "novel":
            sys_prompt = "ä½ æ˜¯ä¸€ä½è·å¾—è¯ºè´å°”æ–‡å­¦å¥–çš„å°è¯´å®¶ã€‚è¯·æ ¹æ®ç”¨æˆ·çš„è¦æ±‚åˆ›ä½œæƒ…èŠ‚è·Œå®•èµ·ä¼ã€æå†™ç»†è…»ã€äººç‰©æ€§æ ¼é²œæ˜çš„å°è¯´ã€‚è¯·ä½¿ç”¨ç”ŸåŠ¨çš„ä¿®è¾æ‰‹æ³•ã€‚"
            self.gen_kwargs["temperature"] = 0.95  # å†™å°è¯´æ›´å‘æ•£
        else:
            sys_prompt = "ä½ æ˜¯ä¸€ä¸ªç²¾é€šç¼–ç¨‹ã€ç§‘å­¦ä¸äººæ–‡çš„ AI åŠ©æ‰‹ã€‚å›ç­”è¦æ¡ç†æ¸…æ™°ï¼Œå‡†ç¡®æ— è¯¯ã€‚å¯ä»¥ä½¿ç”¨ Markdown æ ¼å¼ä¼˜åŒ–æ’ç‰ˆã€‚"
            self.gen_kwargs["temperature"] = 0.7   # é—®ç­”æ›´ä¸¥è°¨
        
        self.messages = [{"role": "system", "content": sys_prompt}]
        console.print(f"[dim]å·²é‡ç½®ä¸Šä¸‹æ–‡ï¼Œå½“å‰æ¨¡å¼: {self.mode}[/dim]")

    def trim_history(self):
        """æ»‘åŠ¨çª—å£ï¼šå½“å¯¹è¯è¿‡é•¿æ—¶ï¼Œç§»é™¤æœ€æ—©çš„å¯¹è¯ï¼ˆä¿ç•™ System Promptï¼‰"""
        # System prompt æ˜¯ index 0ï¼Œæ‰€ä»¥æˆ‘ä»¬æ£€æŸ¥é•¿åº¦æ˜¯å¦è¶…è¿‡ limit + 1
        if len(self.messages) > (self.max_history_len * 2) + 1:
            # ä¿ç•™ system prompt (index 0)ï¼Œåˆ‡æ‰ä¸­é—´æ—§çš„ï¼Œä¿ç•™æœ€è¿‘çš„
            removed_count = len(self.messages) - ((self.max_history_len * 2) + 1)
            self.messages = [self.messages[0]] + self.messages[-(self.max_history_len * 2):]
            console.print(f"[dim yellow]âš ï¸ä¸ºäº†ä¿æŒæ€ç»´æ¸…æ™°ï¼Œé—å¿˜äº† {removed_count} æ¡æ—§æ¶ˆæ¯...[/dim yellow]")

    def save_chat(self, filename="chat_history.json"):
        """ä¿å­˜å¯¹è¯åˆ°æœ¬åœ°"""
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(self.messages, f, ensure_ascii=False, indent=2)
            console.print(f"[green]âœ… å¯¹è¯å·²ä¿å­˜è‡³ {filename}[/green]")
        except Exception as e:
            console.print(f"[red]âŒ ä¿å­˜å¤±è´¥: {e}[/red]")

    def load_chat(self, filename="chat_history.json"):
        """åŠ è½½æœ¬åœ°å¯¹è¯"""
        if not os.path.exists(filename):
            console.print(f"[red]âŒ æ‰¾ä¸åˆ°æ–‡ä»¶: {filename}[/red]")
            return
        try:
            with open(filename, 'r', encoding='utf-8') as f:
                self.messages = json.load(f)
            console.print(f"[green]âœ… å·²åŠ è½½å†å²å¯¹è¯ ({len(self.messages)} æ¡æ¶ˆæ¯)[/green]")
        except Exception as e:
            console.print(f"[red]âŒ åŠ è½½å¤±è´¥: {e}[/red]")

    def chat(self, user_input):
        self.trim_history() # æ£€æŸ¥æ˜¯å¦éœ€è¦é—å¿˜æ—§æ¶ˆæ¯
        self.messages.append({"role": "user", "content": user_input})
        
        text = self.tokenizer.apply_chat_template(
            self.messages,
            tokenize=False,
            add_generation_prompt=True
        )
        model_inputs = self.tokenizer([text], return_tensors="pt").to(self.model.device)

        # æ‰“å° AI æ€è€ƒä¸­çš„æç¤º
        console.print(Text("ğŸ¤– AI æ­£åœ¨æ€è€ƒ...", style="bold cyan"), end="\r")

        streamer = TextStreamer(self.tokenizer, skip_prompt=True, skip_special_tokens=True)
        
        # æ¢è¡Œå¼€å§‹è¾“å‡º
        print("\n" + "-"*30) 
        
        # è°ƒæ•´æ˜¾å­˜
        if self.device == "cuda":
            torch.cuda.empty_cache()

        generated_ids = self.model.generate(
            **model_inputs,
            streamer=streamer,
            **self.gen_kwargs
        )
        print("-" * 30 + "\n")

        # ä¿å­˜å›å¤
        response = self.tokenizer.decode(generated_ids[0][model_inputs.input_ids.shape[-1]:], skip_special_tokens=True)
        self.messages.append({"role": "assistant", "content": response})

def print_menu():
    menu_text = """
    [bold cyan]ğŸ® æŒ‡ä»¤èœå•:[/bold cyan]
    [green]/novel[/green] - åˆ‡æ¢å°è¯´æ¨¡å¼ (é«˜åˆ›é€ åŠ›)
    [green]/chat[/green]  - åˆ‡æ¢åŠ©æ‰‹æ¨¡å¼ (é«˜ä¸¥è°¨åº¦)
    [green]/save[/green]  - ä¿å­˜å½“å‰å¯¹è¯
    [green]/load[/green]  - è¯»å–å†å²å¯¹è¯
    [green]/temp X[/green]- è®¾ç½®æ¸©åº¦ (0.1-1.0)ï¼Œä¾‹å¦‚ /temp 0.9
    [green]/clear[/green] - æ¸…ç©ºè®°å¿†
    [red]/exit[/red]  - é€€å‡ºç¨‹åº
    """
    console.print(Panel(menu_text, title="SuperChatbot Pro", subtitle="åŸºäº Qwen2.5", border_style="blue"))

def main():
    # å»ºè®®æ ¹æ®æ˜¾å­˜å¤§å°ä¿®æ”¹æ­¤å¤„ï¼Œ8Gæ˜¾å­˜æ¨è 1.5B æˆ– 3B
    model_name = "Qwen/Qwen2.5-0.5B-Instruct" 
    
    bot = SuperChatbot(model_name)
    print_menu()
    
    while True:
        try:
            mode_icon = "ğŸ“" if bot.mode=='novel' else "ğŸ§ "
            user_input = Prompt.ask(f"\n[bold]{mode_icon} ä½ [/bold]")
            
            if not user_input.strip(): continue

            # æŒ‡ä»¤å¤„ç†
            if user_input.startswith("/"):
                cmd_parts = user_input.lower().split()
                cmd = cmd_parts[0]
                
                if cmd == '/exit': break
                elif cmd == '/clear': bot.reset_history()
                elif cmd == '/novel': 
                    bot.mode = "novel"
                    bot.reset_history()
                elif cmd == '/chat': 
                    bot.mode = "assistant"
                    bot.reset_history()
                elif cmd == '/save': bot.save_chat()
                elif cmd == '/load': bot.load_chat()
                elif cmd == '/temp':
                    if len(cmd_parts) > 1:
                        try:
                            val = float(cmd_parts[1])
                            bot.gen_kwargs["temperature"] = max(0.1, min(1.5, val))
                            console.print(f"[dim]ğŸŒ¡ï¸ æ¸©åº¦å·²è®¾ç½®ä¸º: {bot.gen_kwargs['temperature']}[/dim]")
                        except: console.print("[red]âŒ è¯·è¾“å…¥æ•°å­—ï¼Œä¾‹å¦‚ /temp 0.8[/red]")
                    else:
                        console.print(f"[dim]å½“å‰æ¸©åº¦: {bot.gen_kwargs['temperature']}[/dim]")
                else:
                    console.print("[red]âŒ æœªçŸ¥æŒ‡ä»¤[/red]")
                continue

            # æ­£å¸¸å¯¹è¯
            bot.chat(user_input)

        except KeyboardInterrupt:
            console.print("\n[yellow]æ£€æµ‹åˆ°ä¸­æ–­ï¼Œæ­£åœ¨é€€å‡º...[/yellow]")
            break
        except Exception as e:
            console.print(f"[bold red]å‘ç”Ÿé”™è¯¯: {e}[/bold red]")

if __name__ == "__main__":
    main()