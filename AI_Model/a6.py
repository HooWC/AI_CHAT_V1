import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer, BitsAndBytesConfig

class FastNovelWriter:
    def __init__(self):
        # ä¾ç„¶ä½¿ç”¨ 1.5B æ•ˆæœè¾ƒå¥½ï¼Œå¦‚æœè¿½æ±‚æè‡´é€Ÿåº¦å¯ä»¥æ¢å› 0.5B
        self.model_name = "Qwen/Qwen2.5-1.5B-Instruct" 
        print(f"ğŸš€ æ­£åœ¨ä»¥åŠ é€Ÿæ¨¡å¼åŠ è½½å¼•æ“: {self.model_name}...")

        # 1. é…ç½® 4-bit é‡åŒ–ï¼Œè¿™æ˜¯æé€Ÿçš„å…³é”®
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.bfloat16 # å¦‚æœæ˜¾å¡ä¸æ”¯æŒbf16ï¼Œæ”¹ä¸ºtorch.float16
        )

        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        
        # 2. åŠ è½½æ¨¡å‹
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            quantization_config=bnb_config, # åº”ç”¨é‡åŒ–
            device_map="auto",             # è‡ªåŠ¨åˆ†é…æ˜¾å­˜/å†…å­˜
            low_cpu_mem_usage=True,
            trust_remote_code=True
        )
        
        # 3. å¼€å¯æ¨ç†åŠ é€Ÿï¼ˆé’ˆå¯¹æ”¯æŒçš„ç®—å­ï¼‰
        # self.model = self.model.to_bettertransformer() # å¯é€‰ï¼Œè§†ç¯å¢ƒè€Œå®š

        self.system_prompt = (
            "ä½ æ˜¯ä¸€ä½é¡¶çº§çš„ç½‘æ–‡å¤§ç¥ï¼Œæ“…é•¿ç»†è…»çš„å¿ƒç†æå†™ã€ç¯å¢ƒæ¸²æŸ“å’Œæ…¢èŠ‚å¥çš„æƒ…èŠ‚é“ºé™ˆã€‚\n"
            "ã€è§„åˆ™ã€‘ï¼šä¸¥ç¦è·³è¿‡å‰§æƒ…ï¼Œç¦æ­¢åšæ€»ç»“æ€§é™ˆè¿°ï¼Œæ¯ä¸€ç« å¿…é¡»åŒ…å«å¤§é‡çš„ç»†èŠ‚æå†™ï¼ŒèŠ‚å¥è¦æ…¢ã€‚"
        )
        self.messages = []

    def write_long_chapter(self, prompt, target_length=1500):
        self.messages = [{"role": "system", "content": self.system_prompt}]
        self.messages.append({"role": "user", "content": f"è¯·å¼€å§‹åˆ›ä½œå°è¯´ï¼š{prompt}ã€‚æ³¨æ„ï¼šç»†èŠ‚è¦ä¸°å¯Œï¼Œä¸è¦æ€¥äºå®Œç»“ã€‚"})
        
        full_story = ""
        print(f"\nâš¡ é«˜é€Ÿæ¨¡å¼å¼€å¯ï¼Œç›®æ ‡å­—æ•°ï¼š{target_length}...")

        while len(full_story) < target_length:
            # ä½¿ç”¨ apply_chat_template æ„å»ºè¾“å…¥
            text = self.tokenizer.apply_chat_template(self.messages, tokenize=False, add_generation_prompt=True)
            model_inputs = self.tokenizer([text], return_tensors="pt").to(self.model.device)
            
            # ä½¿ç”¨æµå¼è¾“å‡ºï¼Œè¾¹å†™è¾¹çœ‹å°±ä¸ä¼šè§‰å¾—æ…¢äº†
            streamer = TextStreamer(self.tokenizer, skip_prompt=True, skip_special_tokens=True)

            # ç”Ÿæˆé…ç½®ä¼˜åŒ–
            with torch.no_grad(): # ç¦ç”¨æ¢¯åº¦è®¡ç®—ï¼Œçœå†…å­˜æé€Ÿ
                generated_ids = self.model.generate(
                    **model_inputs,
                    streamer=streamer,
                    max_new_tokens=512, # å‡å°å•æ¬¡ç”Ÿæˆé•¿åº¦ï¼Œä¿æŒæ¨ç†é«˜æ•ˆ
                    do_sample=True,
                    temperature=0.8,
                    top_p=0.9,
                    repetition_penalty=1.1,
                    use_cache=True # åŠ¡å¿…å¼€å¯ç¼“å­˜ï¼Œè¿™æ˜¯æé€Ÿæ ¸å¿ƒ
                )
            
            response_ids = generated_ids[0][model_inputs.input_ids.shape[-1]:]
            response_text = self.tokenizer.decode(response_ids, skip_special_tokens=True)
            
            full_story += response_text
            self.messages.append({"role": "assistant", "content": response_text})
            
            if len(full_story) < target_length:
                self.messages.append({"role": "user", "content": "è¯·ç´§æ¥ä¸Šæ–‡ï¼Œç»§ç»­è¯¦ç»†æå†™æƒ…èŠ‚ã€‚"})
            else:
                break
        
        return full_story

def main():
    writer = FastNovelWriter()
    while True:
        user_topic = input("\nğŸ‘¤ è¾“å…¥ä¸»é¢˜: ").strip()
        if user_topic.lower() == 'exit': break
        writer.write_long_chapter(user_topic, target_length=1500)

if __name__ == "__main__":
    main()