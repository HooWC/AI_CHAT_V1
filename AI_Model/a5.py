import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer

class NovelProWriter:
    def __init__(self):
        # å»ºè®®è‡³å°‘ä½¿ç”¨ 1.5B æ¨¡å‹ï¼Œ0.5B çš„é€»è¾‘é“¾å¤ªçŸ­ï¼Œå¾ˆéš¾å†™é•¿æ–‡ä¸è·‘é¢˜
        self.model_name = "Qwen/Qwen2.5-1.5B-Instruct" 
        print(f"æ­£åœ¨åŠ è½½ä¸“ä¸šåˆ›ä½œå¼•æ“: {self.model_name}...")
        
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype="auto",
            device_map="auto"
        )
        
        # æ ¸å¿ƒï¼šæå…¶è¯¦ç»†çš„ç³»ç»Ÿè®¾å®šï¼Œå¼ºè¿« AI æ”¾å¼ƒâ€œæ€»ç»“å¼â€å†™æ³•ï¼Œæ”¹ç”¨â€œæå†™å¼â€å†™æ³•
        self.system_prompt = (
            "ä½ æ˜¯ä¸€ä½é¡¶çº§çš„ç½‘æ–‡å¤§ç¥ï¼Œæ“…é•¿ç»†è…»çš„å¿ƒç†æå†™ã€ç¯å¢ƒæ¸²æŸ“å’Œæ…¢èŠ‚å¥çš„æƒ…èŠ‚é“ºé™ˆã€‚\n"
            "ã€è§„åˆ™ã€‘ï¼š\n"
            "1. ä¸¥ç¦è·³è¿‡å‰§æƒ…ï¼Œç¦æ­¢åšæ€»ç»“æ€§é™ˆè¿°ï¼ˆå¦‚â€œä»–ä»¬ç»å†äº†ä¸€åœºæ¿€æˆ˜â€æ˜¯é”™è¯¯çš„ï¼Œå¿…é¡»å†™å…·ä½“çš„åŠ¨ä½œå’Œå¯¹è¯ï¼‰ã€‚\n"
            "2. æ¯ä¸€ç« å¿…é¡»åŒ…å«å¤§é‡çš„ç¯å¢ƒç»†èŠ‚æå†™å’Œäººç‰©å†…å¿ƒæ´»åŠ¨ã€‚\n"
            "3. èŠ‚å¥è¦æ…¢ï¼Œè¯­è¨€è¦ä¼˜ç¾ä¸”æœ‰æ„ŸæŸ“åŠ›ã€‚\n"
            "4. å¦‚æœæ•…äº‹æ²¡å†™å®Œï¼Œè¯·åœ¨ç»“å°¾ç•™ä¸‹ä¼ç¬”ã€‚"
        )
        self.messages = []

    def write_long_chapter(self, prompt, target_length=1500):
        self.messages = [{"role": "system", "content": self.system_prompt}]
        self.messages.append({"role": "user", "content": f"è¯·å¼€å§‹åˆ›ä½œå°è¯´ï¼š{prompt}ã€‚æ³¨æ„ï¼šè¯·å…ˆå†™ç¬¬ä¸€éƒ¨åˆ†ï¼Œç»†èŠ‚è¦ä¸°å¯Œï¼Œä¸è¦æ€¥äºå®Œç»“ã€‚"})
        
        full_story = ""
        current_step = 1
        
        print(f"\nğŸš€ å¼€å§‹åˆ›ä½œé•¿ç¯‡ç« èŠ‚ï¼Œç›®æ ‡å­—æ•°ï¼š{target_length}...")

        while len(full_story) < target_length:
            print(f"\n--- æ­£åœ¨åˆ›ä½œç¬¬ {current_step} æ®µ (å½“å‰æ€»å­—æ•°: {len(full_story)}) ---")
            
            # æ„å»ºè¾“å…¥
            text = self.tokenizer.apply_chat_template(self.messages, tokenize=False, add_generation_prompt=True)
            model_inputs = self.tokenizer([text], return_tensors="pt").to(self.model.device)
            streamer = TextStreamer(self.tokenizer, skip_prompt=True, skip_special_tokens=True)

            # ç”Ÿæˆè¿™ä¸€æ®µ
            generated_ids = self.model.generate(
                **model_inputs,
                streamer=streamer,
                max_new_tokens=800, # æ¯æ¬¡ç”Ÿæˆçš„ä¸­æ®µé•¿åº¦
                do_sample=True,
                temperature=0.9,     # ç•¥é«˜ä¸€ç‚¹å¢åŠ æ–‡é‡‡
                top_p=0.95,
                repetition_penalty=1.15
            )
            
            response_ids = generated_ids[0][model_inputs.input_ids.shape[-1]:]
            response_text = self.tokenizer.decode(response_ids, skip_special_tokens=True)
            
            # æ‹¼æ¥åˆ°å…¨æ–‡
            full_story += response_text
            self.messages.append({"role": "assistant", "content": response_text})
            
            # æ£€æŸ¥å­—æ•°ï¼Œå¦‚æœä¸å¤Ÿï¼Œè‡ªåŠ¨è¿½åŠ æŒ‡ä»¤
            if len(full_story) < target_length:
                self.messages.append({"role": "user", "content": "è¯·ç»§ç»­ç´§æ¥ä¸Šæ–‡æå†™ï¼Œä¿æŒç»†èŠ‚ä¸°å¯Œï¼Œä¸è¦è·³è·ƒå‰§æƒ…ï¼Œç»§ç»­å†™ã€‚"})
                current_step += 1
            else:
                break
        
        print(f"\nâœ… ç« èŠ‚åˆ›ä½œå®Œæˆï¼æ€»å­—æ•°ï¼š{len(full_story)}")
        return full_story

def main():
    writer = NovelProWriter()
    while True:
        user_topic = input("\nğŸ‘¤ è¾“å…¥å°è¯´ä¸»é¢˜æˆ–å¼€å¤´: ").strip()
        if user_topic.lower() == 'exit': break
        
        # è®¾å®šç›®æ ‡å­—æ•°ä¸º 1500
        chapter_content = writer.write_long_chapter(user_topic, target_length=1500)
        
        save_yn = input("\nğŸ’¾ æ˜¯å¦ä¿å­˜åˆ°æ–‡ä»¶ï¼Ÿ(y/n): ")
        if save_yn.lower() == 'y':
            with open("novel_chapter.txt", "w", encoding="utf-8") as f:
                f.write(chapter_content)
            print("ğŸ“ å·²ä¿å­˜è‡³ novel_chapter.txt")

if __name__ == "__main__":
    main()