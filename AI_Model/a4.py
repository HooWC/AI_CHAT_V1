import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer

class SuperChatbot:
    def __init__(self):
        # å¦‚æœå†…å­˜å…è®¸ï¼Œå»ºè®®æ¢æˆ "Qwen/Qwen2.5-1.5B-Instruct" æ•ˆæœå¥½éå¸¸å¤š
        self.model_name = "Qwen/Qwen2.5-0.5B-Instruct"
        print(f"æ­£åœ¨åŠ è½½å¼•æ“: {self.model_name}...")
        
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype="auto",
            device_map="auto"
        )
        
        # é»˜è®¤æ¨¡å¼
        self.mode = "assistant"
        self.messages = []
        self.reset_history()

    def reset_history(self):
        """é‡ç½®å¯¹è¯ï¼Œæ ¹æ®æ¨¡å¼è®¾å®šä¸åŒçš„ç³»ç»Ÿæç¤ºè¯"""
        if self.mode == "novel":
            prompt = "ä½ æ˜¯ä¸€ä½ç²¾é€šå„ç§é£æ ¼çš„å°è¯´å®¶ã€‚è¯·æ ¹æ®ç”¨æˆ·çš„è¦æ±‚åˆ›ä½œæƒ…èŠ‚ä¸°å¯Œã€æå†™ç”ŸåŠ¨ã€é€»è¾‘è‡ªæ´½çš„å°è¯´ã€‚"
        else:
            prompt = "ä½ æ˜¯ä¸€ä¸ªé€šæ™“ç™¾ç§‘ã€ä¹äºåŠ©äººçš„ä¸­æ–‡ AI åŠ©æ‰‹ã€‚"
        
        self.messages = [{"role": "system", "content": prompt}]

    def chat(self, user_input):
        self.messages.append({"role": "user", "content": user_input})
        
        # æ„å»ºè¾“å…¥
        text = self.tokenizer.apply_chat_template(
            self.messages,
            tokenize=False,
            add_generation_prompt=True
        )
        model_inputs = self.tokenizer([text], return_tensors="pt").to(self.model.device)

        # å®ä¾‹åŒ–æµå¼å™¨ï¼Œè®©æ–‡å­—åœ¨ CMD é‡Œä¸€ä¸ªä¸€ä¸ªè¹¦å‡ºæ¥
        streamer = TextStreamer(self.tokenizer, skip_prompt=True, skip_special_tokens=True)

        print("ğŸ¤– AI: ", end="", flush=True)
        
        # ç”Ÿæˆå‚æ•°ä¼˜åŒ–
        generation_kwargs = dict(
            **model_inputs,
            streamer=streamer,
            max_new_tokens=1024, # å…è®¸ç”Ÿæˆæ›´é•¿çš„æ–‡æœ¬
            do_sample=True,      # å¿…é¡»å¼€å¯é‡‡æ ·æ‰èƒ½å†™å°è¯´
            temperature=0.85,    # æé«˜éšæœºæ€§ï¼Œé¿å…ç”±äºå¤ªæ­»æ¿å¯¼è‡´çš„è¯ç©·
            top_p=0.9,
            repetition_penalty=1.1 # ç¨å¾®åŠ å¤§æƒ©ç½šï¼Œé˜²æ­¢å†™å°è¯´æ—¶åå¤é‡å¤ä¸€æ®µè¯
        )

        # å¯åŠ¨ç”Ÿæˆ
        generated_ids = self.model.generate(**generation_kwargs)
        
        # è·å–å›å¤å†…å®¹å­˜å…¥å†å²
        response = self.tokenizer.decode(generated_ids[0][model_inputs.input_ids.shape[-1]:], skip_special_tokens=True)
        self.messages.append({"role": "assistant", "content": response})

def main():
    bot = SuperChatbot()
    print("\n[æŒ‡ä»¤è¯´æ˜]: è¾“å…¥ 'novel' è¿›å…¥å°è¯´æ¨¡å¼ | 'chat' å›åˆ°é—®ç­”æ¨¡å¼ | 'clear' é‡ç½®")
    
    while True:
        mode_str = f"[{'å†™ä½œ' if bot.mode=='novel' else 'åŠ©æ‰‹'}]"
        user_input = input(f"\nğŸ‘¤ {mode_str} ä½ : ").strip()
        
        if not user_input: continue
        
        if user_input.lower() == 'exit': break
        if user_input.lower() == 'novel':
            bot.mode = "novel"
            bot.reset_history()
            print("âœ¨ å·²åˆ‡æ¢åˆ°å°è¯´åˆ›ä½œæ¨¡å¼ï¼è¯·è¾“å…¥ä½ çš„å°è¯´å¼€å¤´æˆ–è®¾å®šã€‚")
            continue
        if user_input.lower() == 'chat':
            bot.mode = "assistant"
            bot.reset_history()
            print("ğŸ’¡ å·²å›åˆ°æ™®é€šé—®ç­”æ¨¡å¼ã€‚")
            continue
        if user_input.lower() == 'clear':
            bot.reset_history()
            print("ğŸ§¹ è®°å¿†å·²æ¸…ç©ºã€‚")
            continue

        bot.chat(user_input)

if __name__ == "__main__":
    main()