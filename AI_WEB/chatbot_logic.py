import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer
from threading import Thread

class SuperChatbot:
    def __init__(self):
        # é™çº§åˆ° 0.5Bï¼Œè¿™æ˜¯ç›®å‰èƒ½è·‘çš„æœ€è½»é‡ä¸”æœ‰æ™ºå•†çš„ç‰ˆæœ¬
        self.model_id = "Qwen/Qwen2.5-0.5B-Instruct"
        
        print(f"ğŸš€ æ­£åœ¨å¯åŠ¨è½»é‡ç‰ˆå¼•æ“ (Qwen2.5-0.5B)...")
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_id)
        
        # å¼ºåˆ¶ CPU è¿è¡Œï¼Œä¸”å…³é—­æ‰€æœ‰ä¸å¿…è¦çš„åŠ è½½é¡¹
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_id,
            torch_dtype=torch.float32,
            device_map={"": "cpu"} 
        )
        print("âœ… å¼•æ“å¯åŠ¨æˆåŠŸï¼ç°åœ¨ç³»ç»Ÿåº”è¯¥éå¸¸æµç•…ã€‚")

    def chat_stream(self, user_input, history):
        # ç³»ç»Ÿæç¤ºè¯ç¨å¾®åŠ å¼ºï¼Œå¼¥è¡¥æ¨¡å‹å‚æ•°å°çš„ä¸è¶³
        messages = [{"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªç®€æ˜æ‰¼è¦ã€ä¸“ä¸šçš„ AI åŠ©æ‰‹ã€‚"}]
        # 0.5B è®°ä¸ä½å¤ªé•¿çš„ä¸œè¥¿ï¼Œåªä¿ç•™æœ€è¿‘ 2 è½®å¯¹è¯
        messages.extend(history[-4:]) 
        messages.append({"role": "user", "content": user_input})

        text = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        model_inputs = self.tokenizer([text], return_tensors="pt")

        streamer = TextIteratorStreamer(self.tokenizer, skip_prompt=True, skip_special_tokens=True)
        generate_kwargs = dict(
            **model_inputs,
            streamer=streamer,
            max_new_tokens=300, # ç¼©çŸ­å•æ¬¡å›å¤é•¿åº¦ï¼Œè¿›ä¸€æ­¥æå‡é€Ÿåº¦
            do_sample=True,
            temperature=0.7,
            top_p=0.8
        )

        thread = Thread(target=self.model.generate, kwargs=generate_kwargs)
        thread.start()

        for new_text in streamer:
            yield new_text